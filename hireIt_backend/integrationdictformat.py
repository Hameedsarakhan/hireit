# -*- coding: utf-8 -*-
"""parserVersion2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jVVb5jcGyelVooYtCOTU5jYNIOnSvlQx
"""

import warnings
warnings.filterwarnings('ignore')
import en_core_web_sm
nlp=en_core_web_sm.load()
import io
import re
import nltk
import spacy
import en_core_web_sm
import docx2txt
import fitz
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from spacy.matcher import Matcher
from pyresparser import ResumeParser

def extract_information(file_path):
    # def doctotext(m):
    #     temp = docx2txt.process(m)
    #     resume_text = [line.replace('\t', ' ') for line in temp.split('\n') if line]
    #     text = ' '.join(resume_text)
    #     return text

    def pdftotext(m):
        text = ''
        with fitz.open(m) as pdf_document:
            num_pages = pdf_document.page_count
            for page_number in range(num_pages):
                page = pdf_document[page_number]
                text += page.get_text()
        return text

    def extract_edu(resume_text):
        nlp_text = nlp(resume_text)
        edu = ['b.e', 'b.tech', 'b.s', 'be', 'btech', 'bs',
               'bachelor', 'bachelor of science', 'bachelor of engineering',
               'bachelor of technology']
        extracted_edu = []

        for token in nlp_text:
            if token.text.lower() in edu:
                extracted_edu.append(token.text)

        for chunk in nlp_text.noun_chunks:
            chunk_text = chunk.text.lower().strip()
            if chunk_text in edu:
                extracted_edu.append(chunk_text)

        return set([word.capitalize() for word in extracted_edu])

    def extract_school(resume_txt):
      sub_patterns = [
        '[A-Za-zÀ-ȕ]* University',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* University',
        '[A-Za-zÀ-ȕ]* Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute *[A-Za-zÀ-ȕ] *[A-Za-zÀ-ȕ]',
        '[A-Za-zÀ-ȕ]* Institute',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* Institute',
        'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
        'Institute [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
        'Institute [A-Za-zÀ-ȕ]*',
        'University *[A-Za-zÀ-ȕ] [A-Za-zÀ-ȕ]*',
        'University [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]*',
        'University [A-Za-zÀ-ȕ]*',
        '[A-Za-zÀ-ȕ]* School',
        '[A-Za-zÀ-ȕ]* [A-Z][a-z]* School',
        '[A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* [A-Za-zÀ-ȕ]* School']
      pattern = '({})'.format('|'.join(sub_patterns))
      matches = re.findall(pattern, resume_txt)
      unique_matches = set(matches)
      return unique_matches

    def extract_links(text):
        linkedin_pattern = r'linkedin\.com/in/([^\s]+)'
        github_pattern = r'github\.com/([^\s]+)'

        linkedin_links = re.findall(linkedin_pattern, text)
        github_links = re.findall(github_pattern, text)

        return {
            "linkedin_links": linkedin_links,
            "github_links": github_links,
        }

    warnings.filterwarnings('ignore')

    # if file_path.lower().endswith(('.png', '.docx')):
    #     text_input = doctotext(file_path)
    # elif file_path.lower().endswith('.pdf'):
    #     text_input = pdftotext(file_path)
    # else:
    #     return {"error": "File not supported"}
    text_input=pdftotext(file_path)

    extracted_edu = extract_edu(text_input)
    extracted_school = extract_school(text_input)
    extracted_links = extract_links(text_input)

    resume_data = ResumeParser(file_path).get_extracted_data()

    return {
        "education": list(extracted_edu),
        "school": list(extracted_school),
        "skills": resume_data.get("skills", []),
        "experience": resume_data.get("experience", []),
        "name": resume_data.get("name", ""),
        "email": resume_data.get("email", ""),
        "mobile_number": resume_data.get("mobile_number", ""),
        "links": extracted_links,
    }



